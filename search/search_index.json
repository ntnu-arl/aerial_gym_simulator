{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#aerial-gym-simulator","title":"Aerial Gym Simulator","text":"<p>Welcome to the documentation of the Aerial Gym Simulator \u00a0\u00a0 </p> <p>The Aerial Gym Simulator is a high-fidelity physics-based simulator for training Micro Aerial Vehicle (MAV) platforms such as multirotors to learn to fly and navigate cluttered environments using learning-based methods. The environments are built upon the underlying NVIDIA Isaac Gym simulator. We offer aerial robot models for standard planar quadrotor platforms, as well as fully-actuated platforms and multirotors with arbitrary configurations. These configurations are supported with low-level and high-level geometric controllers that reside on the GPU and provide parallelization for the simultaneous control of thousands of multirotors.</p> <p>This is the second release of the simulator and includes a variety of new features and improvements. Task definition and environment configuration allow for fine-grained customization of all the environment entities without having to deal with large monolithic environment files. A custom rendering framework allows obtaining depth, and segmentation images at high speeds and can be used to simulate custom sensors such as LiDARs with varying properties. The simulator is open-source and is released under the BSD-3-Clause License.</p> <p>Aerial Gym Simulator allows you to train state-based control policies in under a minute,</p> <p></p> <p>And train vision-based navigation policies in under an hour:</p> <p></p> <p>Equipped with GPU-accelerated and customizable ray-casting based LiDAR and Camera sensors with depth and segmentation capabilities:</p> <p> </p> <p> </p>"},{"location":"#features","title":"Features","text":"<ul> <li> Modular and Extendable Design   allowing users to easily create custom environments, robots, sensors, tasks, and controllers, and changing parameters programmatically on-the-fly by modifying the Simulation Components. </li> <li> Rewritten from the Ground-Up   to offer very high control over each of the simulation components and capability to extensively customize the simulator to your needs. </li> <li> High-Fidelity Physics Engine   leveraging NVIDIA Isaac Gym, which provides a high-fidelity physics engine for simulating multirotor platforms, with the possibility of adding support for custom physics engine backends and rendering pipelines. </li> <li> Parallelized Geometric Controllers   that reside on the GPU and provide parallelization for the simultaneous control of thousands of multirotor vehicles. </li> <li> Custom Rendering Framework   (based on NVIDIA Warp) used to design custom sensors and perform parallelized kernel-based operations. </li> <li> Modular and Extendable   allowing users to easily create custom environments, robots, sensors, tasks, and controllers. </li> <li> RL-based control and navigation policies   of your choice can be added for robot learning tasks. Includes scripts to get started with training your own robots.. </li> </ul> <p>Support for Isaac Lab</p> <p>Support for Isaac Lab and Isaac Sim is currently under development. We anticipate releasing this feature in the near future.</p> <p>Please refer to the paper detailing the previous version of our simulator to get insights into the motivation and the design principles involved in creating the Aerial Gym Simulator: https://arxiv.org/abs/2305.16510 (link will be updated to reflect the newer version soon!).</p>"},{"location":"#why-aerial-gym-simulator","title":"Why Aerial Gym Simulator?","text":"<p>The Aerial Gym Simulator is designed to simulate thousands of MAVs simultaneously and comes equipped with both low and high-level controllers that are used on real-world systems. In addition, the new customized ray-casting allows for superfast rendering of the environment for tasks using depth and segmentation from the environment.</p> <p>The optimized code in this newer version allows training for motor-command policies for robot control in under a minute and vision-based navigation policies in under an hour. Extensive examples are provided to allow users to get started with training their own policies for their custom robots quickly.</p>"},{"location":"#citing","title":"Citing","text":"<p>When referencing the Aerial Gym Simulator in your research, please cite the following paper</p> <pre><code>@misc{kulkarni2023aerialgymisaac,\n      title={Aerial Gym -- Isaac Gym Simulator for Aerial Robots},\n      author={Mihir Kulkarni and Theodor J. L. Forgaard and Kostas Alexis},\n      year={2023},\n      eprint={2305.16510},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO},\n      url={https://arxiv.org/abs/2305.16510},\n}\n</code></pre> <p>If you use the reinforcement learning policy provided alongside this simulator for navigation tasks, please cite the following paper:</p> <pre><code>@misc{kulkarni2024reinforcementlearningcollisionfreeflight,\n      title={Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding},\n      author={Mihir Kulkarni and Kostas Alexis},\n      year={2024},\n      eprint={2402.03947},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO},\n      url={https://arxiv.org/abs/2402.03947},\n}\n</code></pre>"},{"location":"#quick-links","title":"Quick Links","text":"<p>For your convenience, here are some quick links to the most important sections of the documentation:</p> <ul> <li>Installation</li> <li>Robots and Controllers</li> <li>Sensors and Rendering Capabilities</li> <li>RL Training</li> <li>Simulation Components</li> <li>Customization</li> <li>Sim2Real Deployment</li> <li>FAQs and Troubleshooting</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>Mihir Kulkarni  \u00a0\u00a0\u00a0 Email GitHub LinkedIn X (formerly Twitter)</p> <p>Welf Rehberg \u00a0\u00a0\u00a0\u00a0 Email GitHub LinkedIn</p> <p>Theodor J. L. Forgaard \u00a0\u00a0\u00a0 Email GitHb LinkedIn</p> <p>Kostas Alexis \u00a0\u00a0\u00a0\u00a0 Email GitHub LinkedIn X (formerly Twitter)</p> <p>This work is done at the Autonomous Robots Lab, Norwegian University of Science and Technology (NTNU). For more information, visit our Website.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This material was supported by RESNAV (AFOSR Award No. FA8655-21-1-7033) and SPEAR (Horizon Europe Grant Agreement No. 101119774).</p> <p>This repository utilizes some of the code and helper scripts from https://github.com/leggedrobotics/legged_gym and IsaacGymEnvs.</p>"},{"location":"#faqs-and-troubleshooting","title":"FAQs and Troubleshooting","text":"<p>Please refer to our website or to the Issues section in the GitHub repository for more information.</p>"},{"location":"2_getting_started/","title":"Getting Started","text":""},{"location":"2_getting_started/#installation","title":"Installation","text":"<ol> <li>Download Isaac Gym Preview 4 Release</li> <li> <p>Use the below instructions to install the Isaac Gym simulator:</p> <ol> <li>Install a new conda environment and activate it     <pre><code>conda create -n aerialgym python=3.8\nconda activate aerialgym\n</code></pre></li> <li>Install the dependencies     <pre><code>conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 \\\n pytorch-cuda=11.7 -c pytorch -c conda-forge -c defaults\nconda install pyyaml==6.0 tensorboard==2.13.0 -c conda-forge -c pytorch -c defaults -c nvidia\n# OR the newest version of PyTorch with CUDA version that supported by your driver if you like\n# conda install pytorch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 pytorch-cuda=11.8 -c pytorch -c nvidia\nconda install -c fvcore -c iopath -c conda-forge fvcore iopath\nconda install -c pytorch3d pytorch3d\n</code></pre></li> <li>Install Isaac Gym and dependencies     <pre><code>cd &lt;isaacgym_folder&gt;/python\npip3 install -e .\n# set the environment variables for Isaac Gym\nexport LD_LIBRARY_PATH=~/miniconda3/envs/aerialgym/lib\n# OR\nexport LD_LIBRARY_PATH=~/anaconda3/envs/aerialgym/lib\n\n# if you get an error message \"rgbImage buffer error 999\"\n# then set this environment variable\nexport VK_ICD_FILENAMES=/usr/share/vulkan/icd.d/nvidia_icd.json\n\n# Please add this your .bashrc or .zshrc\n# file to avoid setting the environment variables\n# every time you want to run the simulator\n</code></pre></li> <li>Test the installation     <pre><code>cd &lt;isaacgym_folder&gt;/python/examples\npython3 1080_balls_of_solitude.py\n</code></pre> If the above example runs without any errors, the Isaac Gym installation is successful.</li> </ol> <p>Note \"Change argument parser in Isaac Gym's <code>gymutil.py</code>\" Before installing the Aerial Gym Simulator, a change needs to be made to the Isaac Gym installation. The argument parser in Isaac Gym interferes with the rest of the arguments that may be needed for other learning frameworks. This can be changed by changing line 337 of <code>gymutil.py</code> in the <code>isaacgym</code> folder from <pre><code>args = parser.parse_args()\n</code></pre> to <pre><code>args, _ = parser.parse_known_args()\n</code></pre> 1. Create workspace directory and install dependency in the conda environment <pre><code>mkdir -p ~/workspaces/aerial_gym_ws/src &amp;&amp; cd ~/workspaces/aerial_gym_ws/src\n# Need to install urdfpy package from source owing to unresolved\n# issues with cylinder meshes representation with the pip-distributed release.\n# More info here: https://github.com/mmatl/urdfpy/issues/20\ngit clone git@github.com:mmatl/urdfpy.git\ncd urdfpy\npip3 install -e .\n</code></pre></p> </li> <li> <p>Download and install Aerial Gym Simulator     <pre><code>cd ~/workspaces/aerial_gym_ws/src\ngit clone git@github.com:ntnu-arl/aerial_gym_simulator.git\n# or over HTTPS\n# git clone https://github.com/ntnu-arl/aerial_gym_simulator.git\n\ncd aerial_gym_simulator\npip3 install -e .\n</code></pre></p> </li> <li>Test an example environment     <pre><code>cd ~/workspaces/aerial_gym_ws/src/aerial_gym/aerial_gym/examples\npython3 position_control_example.py\n</code></pre></li> </ol>"},{"location":"2_getting_started/#running-the-examples","title":"Running the examples","text":""},{"location":"2_getting_started/#basic-environment-example","title":"Basic environment example","text":""},{"location":"2_getting_started/#position-control-task-example","title":"Position control task example","text":"<pre><code>cd ~/workspaces/aerial_gym_ws/src/aerial_gym_simulator/examples\npython3 position_control_example.py\n</code></pre> Code for Position Control Example <pre><code>from aerial_gym.utils.logging import CustomLogger\nlogger = CustomLogger(__name__)\nfrom aerial_gym.sim.sim_builder import SimBuilder\nimport torch\n\nif __name__ == \"__main__\":\n    env_manager = SimBuilder().build_env(\n        sim_name=\"base_sim\",\n        env_name=\"empty_env\",\n        robot_name=\"base_quadrotor\",\n        controller_name=\"lee_position_control\",\n        args=None,\n        device=\"cuda:0\",\n        num_envs=64,\n        headless=False,\n        use_warp=False # since there is not supposed to be a camera in the robot for this example.\n    )\n    actions = torch.zeros((env_manager.num_envs, 4)).to(\"cuda:0\")\n    env_manager.reset()\n    for i in range(10000):\n        if i % 500 == 0:\n            logger.info(f\"Step {i}, changing target setpoint.\")\n            actions[:, 0:3] = 2.0 * (torch.rand_like(actions[:, 0:3]) * 2 - 1)\n            actions[:, 3] = torch.pi * (torch.rand_like(actions[:, 3]) * 2 - 1)\n        env_manager.step(actions=actions)\n</code></pre> <p>The above example demonstrates how to create an empty simulation environment, select a quadrotor robot, and control the robot using a geometric position controller. In this example, the <code>action</code> variable is sent to the robot as the commanded position and yaw setpoint to track. This is changed every 100 iterations to a random position and yaw setpoint. The simulation is rendered at each iteration.</p>"},{"location":"2_getting_started/#rendering-and-saving-images","title":"Rendering and Saving Images","text":"<pre><code>cd &lt;path_to_aerial_gym_simulator&gt;/examples\npython3 save_camera_stream.py\n</code></pre> Code for Rendering and Saving Images <pre><code>import numpy as np\nfrom aerial_gym.utils.logging import CustomLogger\n\nlogger = CustomLogger(__name__)\nfrom aerial_gym.sim.sim_builder import SimBuilder\nfrom PIL import Image\nimport matplotlib\nimport torch\n\nif __name__ == \"__main__\":\n    logger.debug(\"this is how a debug message looks like\")\n    logger.info(\"this is how an info message looks like\")\n    logger.warning(\"this is how a warning message looks like\")\n    logger.error(\"this is how an error message looks like\")\n    logger.critical(\"this is how a critical message looks like\")\n\n    env_manager = SimBuilder().build_env(\n        sim_name=\"base_sim\",\n        env_name=\"env_with_obstacles\",  # \"forest_env\", #\"empty_env\", # empty_env\n        robot_name=\"base_quadrotor\",\n        controller_name=\"lee_position_control\",\n        args=None,\n        device=\"cuda:0\",\n        num_envs=2,\n        headless=False,\n        use_warp=True,\n    )\n    actions = torch.zeros((env_manager.num_envs, 4)).to(\"cuda:0\")\n\n    env_manager.reset()\n    seg_frames = []\n    depth_frames = []\n    merged_image_frames = []\n    for i in range(10000):\n        if i % 100 == 0 and i &gt; 0:\n            print(\"i\", i)\n            env_manager.reset()\n            # save frames as a gif:\n            seg_frames[0].save(\n                f\"seg_frames_{i}.gif\",\n                save_all=True,\n                append_images=seg_frames[1:],\n                duration=100,\n                loop=0,\n            )\n            depth_frames[0].save(\n                f\"depth_frames_{i}.gif\",\n                save_all=True,\n                append_images=depth_frames[1:],\n                duration=100,\n                loop=0,\n            )\n            merged_image_frames[0].save(\n                f\"merged_image_frames_{i}.gif\",\n                save_all=True,\n                append_images=merged_image_frames[1:],\n                duration=100,\n                loop=0,\n            )\n            seg_frames = []\n            depth_frames = []\n            merged_image_frames = []\n        env_manager.step(actions=actions)\n        env_manager.render(render_components=\"sensors\")\n        # reset envs that have crashed\n        env_manager.reset_terminated_and_truncated_envs()\n        try:\n            image1 = (\n                255.0 * env_manager.global_tensor_dict[\"depth_range_pixels\"][0, 0].cpu().numpy()\n            ).astype(np.uint8)\n            seg_image1 = env_manager.global_tensor_dict[\"segmentation_pixels\"][0, 0].cpu().numpy()\n        except Exception as e:\n            logger.error(\"Error in getting images\")\n            logger.error(\"Seems like the image tensors have not been created yet.\")\n            logger.error(\"This is likely due to absence of a functional camera in the environment\")\n            raise e\n        seg_image1[seg_image1 &lt;= 0] = seg_image1[seg_image1 &gt; 0].min()\n        seg_image1_normalized = (seg_image1 - seg_image1.min()) / (\n            seg_image1.max() - seg_image1.min()\n        )\n\n        # set colormap to plasma in matplotlib\n        seg_image1_normalized_plasma = matplotlib.cm.plasma(seg_image1_normalized)\n        seg_image1 = Image.fromarray((seg_image1_normalized_plasma * 255.0).astype(np.uint8))\n\n        depth_image1 = Image.fromarray(image1)\n        image_4d = np.zeros((image1.shape[0], image1.shape[1], 4))\n        image_4d[:, :, 0] = image1\n        image_4d[:, :, 1] = image1\n        image_4d[:, :, 2] = image1\n        image_4d[:, :, 3] = 255.0\n        merged_image = np.concatenate((image_4d, seg_image1_normalized_plasma * 255.0), axis=0)\n        # save frames to array:\n        seg_frames.append(seg_image1)\n        depth_frames.append(depth_image1)\n        merged_image_frames.append(Image.fromarray(merged_image.astype(np.uint8)))\n</code></pre> <p>The robot sensors can be accessed through the <code>global_tensor_dict</code> attribute of the <code>env_manager</code>. In this example, the depth/range and segmentation images are saved as gifs every 100 iterations. The depth image is saved as a grayscale image, and the segmentation image is saved as a color image using the plasma colormap from matplotlib. The merged image is saved as a gif with the depth image in the top half and the segmentation image in the bottom half.</p> <p>The created gifs from the sensor streams from the camera and LiDAR sensors are saved as below:</p> <p> </p> <p> </p>"},{"location":"2_getting_started/#rl-environment-example","title":"RL environment example","text":"<pre><code>cd &lt;path_to_aerial_gym_simulator&gt;/examples\npython3 rl_env_example.py\n</code></pre> Code for RL interface example <pre><code>import time\nfrom aerial_gym.utils.logging import CustomLogger\n\nlogger = CustomLogger(__name__)\nfrom aerial_gym.registry.task_registry import task_registry\nimport torch\n\nif __name__ == \"__main__\":\n    logger.print_example_message()\n    start = time.time()\n    rl_task_env = task_registry.make_task(\n        \"position_setpoint_task\",\n        # other params are not set here and default values from the task config file are used\n    )\n    rl_task_env.reset()\n    actions = torch.zeros(\n        (\n            rl_task_env.sim_env.num_envs,\n            rl_task_env.sim_env.robot_manager.robot.controller_config.num_actions,\n        )\n    ).to(\"cuda:0\")\n    actions[:] = 0.0\n    with torch.no_grad():\n        for i in range(10000):\n            if i == 100:\n                start = time.time()\n            obs, reward, terminated, truncated, info = rl_task_env.step(actions=actions)\n    end = time.time()\n</code></pre>"},{"location":"3_robots_and_controllers/","title":"Robots and Controllers","text":""},{"location":"3_robots_and_controllers/#robots","title":"Robots","text":""},{"location":"3_robots_and_controllers/#underactuated-quadrotor","title":"Underactuated Quadrotor","text":"<p>We provide a classic planar quadrotor design as the default choice for the robot platform in our simulation. The robot model is deliberately kept simplistic (in terms of structure, texture etc) to allow for faster simulation speeds. The robot mass and inertia parameters can be set by modifying the URDF of the robot. Moreover, the controller parameters can be tuned to match the response of your real-world platform.</p> <p></p>"},{"location":"3_robots_and_controllers/#fully-acutated-octarotor","title":"Fully-acutated Octarotor","text":"<p>We also provide a fully actuated octarotor platform based on the work in Design, Modeling and Control of an Omni-Directional Aerial Vehicle by Dario Brescianini and Raffaello D\u2019Andrea.</p> <p></p>"},{"location":"3_robots_and_controllers/#arbitrary-configuration","title":"Arbitrary Configuration","text":"<p>To show how to operate with arbitrary platform designs, we also provide a URDF of a multirotor platform with an unconventional configuration.</p> <p></p> <p>The above GIF shows the arbitrary platform trained using a motor-control policy to reach a goal position. </p> <p>Using provided geometric controllers for this configuration</p> <p>will perform suboptimally as they assume that the center of mass for this configuration is at the root link of the robot. To account for the torque exerted about the root link because of an offset center-of-mass, additional terms will be required to be added to the controller. Users are encouraged to provide these modifications themselves for their arbitrary configurations. The controllers will work but will not exploit the controllability of the platform along other dimensions of motion. It is currently an area of active research and users are encouraged to explore learning-based solutions as well.</p> <p>Control allocation for this configuration</p> <p>is derived assuming unconstrained motor command allocation and then clamping the actions. This is a general problem when dealing with arbitrary configurations and is an area of active research. Users are encouraged to explore learning-based solutions for such platforms. The allocation matrix for the provided configurations are calculated w.r.t the root link of the robot.</p>"},{"location":"3_robots_and_controllers/#controllers","title":"Controllers","text":""},{"location":"3_robots_and_controllers/#parallelized-geometric-controllers","title":"Parallelized Geometric Controllers","text":"<p>We adapt and package controllers for underactuated planar platforms such as quadrotor, hexarotor, and octarotor. The controllers are based on the work in Control of Complex Maneuvers for a Quadrotor UAV using Geometric Methods on SE(3) by Taeyoung Lee, Melvin Leok, and N. Harris McClamroch.  We adapt the controllers to provide efficient parallelization on the GPU for the simultaneous control of thousands of multirotor vehicles. The controllers are implemented in PyTorch and pre-compiled using the PyTorch JIT compiler for added speedup. The controllers are designed to be modular and can be easily extended to other platforms. We provide examples of use of this controller with a fully actuated octarotor platform and a planar quadrotor platform.</p> Use of parallelized geometric controllers with fully actuated platforms <p>is bound to produce sub-optimal results as the controllers are designed for quadrotor-like platforms that have all the axes of the motors parallel to each other. The controllers will work on fully actuated platforms but will not exploit the controllability of the platform along other dimensions of motion. It is currently an area of active research and users are encouraged to explore learning-based solutions as well.</p>"},{"location":"3_robots_and_controllers/#attitude-thrust-and-body-rate-thrust-controllers","title":"Attitude-Thrust and Body-rate-Thrust Controllers","text":"<p>The errors in desired orientation \\(e_R\\) and body rates \\(e_{\\Omega}\\) are computed as:</p> \\[ e_R = \\frac{1}{2}  (R_d^T  R - R^T  R_d)^{\\vee}, \\] <p>and</p> \\[ e_{\\Omega} = \\Omega - R^T  R_d  \\Omega_d .\\] <p>The desired body-moment \\(M\\) is computed as:</p> \\[ M = -k_R e_R - k_{\\Omega} e_{\\Omega} + \\Omega \\times J \\Omega, \\] <p>where we set the desired angular velocity \\(\\Omega_d = 0\\) in case of attitude control and \\(R_d = R\\) in case of body-rate control. The thrust command is directly provided as a control input to the controller.</p>"},{"location":"3_robots_and_controllers/#position-velocity-and-acceleration-controllers","title":"Position, Velocity and Acceleration Controllers","text":"<p>Given the above attitude controllers, we first calculate the desired body force \\(f\\) and the desired orientation \\(R_d\\) using the following equations:</p> \\[ f = -k_x e_x - k_v e_v - m g e_3 + m \\ddot{x}_d, \\] <p>and</p> \\[ M = -k_R e_R - k_{\\Omega} e_{\\Omega} + \\Omega \\times J \\Omega_c, \\] <p>where \\(e_R = \\frac{1}{2} (R_c^T R - R^T R_c)^{\\vee}\\), and \\(e_{\\Omega_c} = \\Omega - R^T R_c \\Omega_c\\). To calculate the matrix \\(R_c\\), we use:</p> \\[ R_c = [ b_{1_c}; b_{3_c}\\times b_{1_c}; b_{3_c} ], \\] <p>where, \\(b_{3_c} = - \\frac{-k_x e_x - k_v e_v - m g e_3 + m \\ddot{x}_d}{ || -k_x e_x - k_v e_v - m g e_3 + m \\ddot{x}_d || }\\), and \\(b_{1_c}\\) is a vector orthogonal to \\(b_{3_c}\\).</p> <p>For the case of velocity control, we set the position error \\(e_x = 0\\) and the desired acceleration \\(\\ddot{x}_d = 0\\), for the case of acceleration control, we set the position error and the desired velocity \\(\\dot{x}_d = 0\\).</p> <p>Similarly, for the velocity-steering angle-controller, we combine the above concepts to allow a yaw-setpoint to be provided along with the velocity setpoint.</p>"},{"location":"3_robots_and_controllers/#motor-command-allocation-and-thrust-mapping","title":"Motor Command Allocation and Thrust Mapping","text":"<p>We provide support to allocate motor forces using the control allocation matrix. The allocation matrix is obtained from the geometry of the robot and needs to be explicitly defined in the configuration file. The allocation matrix allows for assigning motor forces given wrench (force and torque) commands for the base link.</p> The allocation matrix for each robot can be defined in the robot configuration file. <p>Below is the example from the fully-actuated octarotor configuration file: <pre><code>allocation_matrix = [\n    [-0.78867513, 0.21132487, -0.21132487, 0.78867513, 0.78867513, -0.21132487, 0.21132487, -0.78867513,],\n    [0.21132487, 0.78867513, -0.78867513, -0.21132487, -0.21132487, -0.78867513, 0.78867513, 0.21132487,],\n    [0.57735027, -0.57735027, -0.57735027, 0.57735027, 0.57735027, -0.57735027, -0.57735027, 0.57735027,],\n    [-0.01547005, -0.25773503, 0.21547005, -0.14226497, 0.14226497, -0.21547005, 0.25773503, 0.01547005,],\n    [-0.21547005, -0.14226497, -0.01547005, 0.25773503, -0.25773503, 0.01547005, 0.14226497, 0.21547005,],\n    [0.23094011, -0.11547005, -0.23094011, 0.11547005, -0.11547005, 0.23094011, 0.11547005, -0.23094011,],\n]\n</code></pre></p> <p>You can read more about how this concept is applied in this blog post.</p> <p>Given an allocation matrix and the motor commands, we can calculate the wrench (force and torque) commands for the base link as </p> \\[ \\begin{bmatrix} f_x \\\\ f_y \\\\ f_z \\\\ M_x \\\\ M_y \\\\ M_z \\end{bmatrix} = A_{6 \\times n}  \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ ... \\\\ u_n \\end{bmatrix}, \\] <p>where \\(f\\) is the force, and \\(M\\) is the moment for the base link, \\(A\\) is the allocation matrix, and \\(u_i\\) represents the motor force for the motor \\(i\\). We use the pseudo-inverse of the allocation matrix to obtain motor commands:</p> \\[ \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ ... \\\\ u_n \\end{bmatrix} = A^+  \\begin{bmatrix} f_x \\\\ f_y \\\\ f_z \\\\ M_x \\\\ M_y \\\\ M_z \\end{bmatrix}_{desired}, \\] <p>where \\(A^+\\) is the pseudo-inverse of the allocation matrix.</p> <p>While this works in the case of platforms such as the quadrotor or fully-actuated octarotor, it may or may not work for arbitrary configurations or it may not produce efficient flight. Users are encouraged to explore learning-based solutions for such platforms.</p>"},{"location":"3_robots_and_controllers/#motor-model","title":"Motor Model","text":"<p>We model the simulated motors as a first-order system with randomizable time-constants. The thrust commands can be clamped to minimum and maximum values and are updated at each time-step based on the reference thrust values from the controller, following the equation:</p> \\[ \\dot{f_i} = \\frac{1}{\\tau_i} (f_{ref_i} - f_i), \\] <p>where \\(f_i\\) is the current thrust value for motor \\(i\\), \\(f_{ref_i}\\) is the reference thrust value from the controller, and \\(\\tau_i\\) is the time-constant of the motor. The thrust rate is clamped to a maximum value to prevent the motors from saturating. Users are encouraged to match this model to their specific robot platform for accurate simulation results.</p> <p>First order motor model</p> <pre><code>def update_motor_thrusts(self, ref_thrust):\n    ref_thrust = torch.clamp(ref_thrust, self.min_thrust, self.max_thrust)\n    self.motor_thrust_rate[:] = (1.0 / self.motor_time_constants) * (ref_thrust - self.current_motor_thrust)\n    self.motor_thrust_rate[:] = torch.clamp(self.motor_thrust_rate, -self.cfg.max_thrust_rate, self.cfg.max_thrust_rate)\n    self.current_motor_thrust[:] = (self.current_motor_thrust + self.dt * self.motor_thrust_rate)\n    return self.current_motor_thrust\n</code></pre>"},{"location":"3_robots_and_controllers/#drag-model","title":"Drag Model","text":"<p>A simplistic model for drag is implemented that that can be customized to the user's needs. Linear and quadratic drag coefficients for drag induced by velocity and angular velocity can be set in the configuration file. The drag force \\(F_{drag}\\) and torque \\(M_{drag}\\) in the body-frame are calculated as:</p> \\[ F_{drag} = -k_{v_{linear}} v -k_{v_{quadratic}} v |v|, \\] <p>and </p> \\[ M_{drag} = -k_{\\omega_{linear}} \\omega -k_{\\omega_{quadratic}} \\omega |\\omega|, \\] <p>where \\(v\\) is the linear velocity, \\(\\omega\\) is the angular velocity expressed in the body frame, and \\(k\\) are the drag coefficients. The drag model can be customized to the user's needs by changing the robot-specific parameters in the configuration file.</p>"},{"location":"4_simulation_components/","title":"Simulation Components","text":"<p>The simulator is made of composable entities that allow changing the physics engine parameters, adding various environment handling functionalities to manipulate the environment at runtime, load the assets, select robots and controllers and design custom sensors.</p>"},{"location":"4_simulation_components/#registries","title":"Registries","text":"<p>The code contains registries for having a named mapping to various configuration files and classes. The registries allow ease of configuration through parameters and allow to mix-and-match various settings, robots, environments, sensors within the simulation. New configurations can be created on-the-fly and registered to be used programmatically by the active code, without having to stop to manually configure the simulation. </p> <p>The code contains registries for the following components:</p> <ul> <li>Simulation Components<ul> <li>Registries</li> <li>Simulation Parameters</li> <li>Assets</li> <li>Environments</li> <li>Tasks</li> <li>Robots</li> <li>Controllers</li> <li>Sensors</li> </ul> </li> </ul> <p>TO register your own custom components, please take a look at the Customization page.</p>"},{"location":"4_simulation_components/#simulation-parameters","title":"Simulation Parameters","text":"<p>We use NVIDIA's Isaac Gym as the simulation engine. The simulator allows for selection of different physics backends, such as PhysX and Flex. The simulator provides a set of APIs to interact with the environment, such as setting the gravity, time step, and rendering options. We provide a set of default configurations for the physics engine (based on PhysX) that can be set as per the user's needs. The simulation params are set here: </p> Default simulation parameters <pre><code>class BaseSimConfig:\n# viewer camera:\nclass viewer:\n    headless = False\n    ref_env = 0\n    camera_position = [-5, -5, 4]  # [m]\n    lookat = [0, 0, 0]  # [m]\n    camera_orientation_euler_deg = [0, 0, 0]  # [deg]\n    camera_follow_type = \"FOLLOW_TRANSFORM\"\n    width = 1280\n    height = 720\n    max_range = 100.0  # [m]\n    min_range = 0.1\n    horizontal_fov_deg = 90\n    use_collision_geometry = False\n    camera_follow_transform_local_offset = [-1.0, 0.0, 0.2]  # m\n    camera_follow_position_global_offset = [-1.0, 0.0, 0.4]  # m\n\nclass sim:\n    dt = 0.01\n    substeps = 1\n    gravity = [0.0, 0.0, -9.81]  # [m/s^2]\n    up_axis = 1  # 0 is y, 1 is z\n    use_gpu_pipeline = True\n\n    class physx:\n        num_threads = 10\n        solver_type = 1  # 0: pgs, 1: tgs\n        num_position_iterations = 4\n        num_velocity_iterations = 2\n        contact_offset = 0.002  # [m]\n        rest_offset = 0.001  # [m]\n        bounce_threshold_velocity = 0.1  # 0.5 [m/s]\n        max_depenetration_velocity = 1.0\n        max_gpu_contact_pairs = 2**24  # 2**24 -&gt; needed for 8000 envs and more\n        default_buffer_size_multiplier = 10\n        contact_collection = 1  # 0: never, 1: last sub-step, 2: all sub-steps (default=2)\n</code></pre> <p>While the default setting of the physics engine is PhysX for this simulator, this can be changed to Flex with minor modifications. It has not been tested for this purpose.</p>"},{"location":"4_simulation_components/#assets","title":"Assets","text":"<p>Simulation assets in Aerial Gym are generally URDF files that can each have their own parameters for simulation. The asset configuration files for assets are stored in <code>config/asset_config</code>. In our implementation we have defined asset classes based on the type of the asset of our purposes and ground their properties together. Assets of each type can be represented through different URDF files inherently enabling randomization across environments without a further need to specify which asset is to be loaded. Just adding additional URDFs to the appropriate folder path suffices to add them in the pool for selection fr simulation.</p> <p>The parameters for each asset are derived from the <code>BaseAssetParams</code> class that includes the number of assets of that type to be loaded per environment, specifies root asset folder, specifies the position, orientation ratios and physics properties of the asset such as damping coefficients, density etc. Additional parameters can be used to specify properties such as presence of force sensors on the asset, per-link or whole body segmentation labels, etc.</p> <p>The <code>BaseAssetParams</code> file is as follows:</p> Example of configurable <code>BaseAssetParams</code> class <pre><code>class BaseAssetParams:\n    num_assets = 1  # number of assets to include per environment\n\n    asset_folder = f\"{AERIAL_GYM_DIRECTORY}/resources/models/environment_assets\"\n    file = None  # if file=None, random assets will be selected. If not None, this file will be used\n\n    min_position_ratio = [0.5, 0.5, 0.5]  # min position as a ratio of the bounds\n    max_position_ratio = [0.5, 0.5, 0.5]  # max position as a ratio of the bounds\n\n    collision_mask = 1\n\n    disable_gravity = False\n    replace_cylinder_with_capsule = True  # replace collision cylinders with capsules, leads to faster/more stable simulation\n    flip_visual_attachments = (\n        True  # Some .obj meshes must be flipped from y-up to z-up\n    )\n    density = 0.000001\n    angular_damping = 0.0001\n    linear_damping = 0.0001\n    max_angular_velocity = 100.0\n    max_linear_velocity = 100.0\n    armature = 0.001\n\n    collapse_fixed_joints = True\n    fix_base_link = True\n    color = None\n    keep_in_env = False\n\n    body_semantic_label = 0\n    link_semantic_label = 0\n    per_link_semantic = False\n    semantic_masked_links = {}\n    place_force_sensor = False\n    force_sensor_parent_link = \"base_link\"\n    force_sensor_transform = [\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n        1.0,\n    ]  # position, quat x, y, z, w\n    use_collision_mesh_instead_of_visual = False\n</code></pre>"},{"location":"4_simulation_components/#environments","title":"Environments","text":"<p>Environment specification determines what are the components in a simulation environment. A configuration file can be used to select a particular robot (with it's sensors), a specified controller for thr robot and selection of obstacles that are present in the environment alongside a strategy to spawn and randomize their positions w.r.t the environment bounds. The environment configuration files are stored in <code>config/env_config</code> folder. The environment manager calls each of the environment entities to allow specific user-coded behaviors at each timestep or to perform certain actions when the environment is reset. The environment manager is responsible for spawning the assets, robots and obstacles in the environment and managing their interactions.</p> Example of an Environment Configuration for an empty environment with a robot <pre><code>class EmptyEnvCfg:\n    class env:\n        num_envs = 3  # number of environments\n        num_env_actions = 0  # this is the number of actions handled by the environment\n        # these are the actions that are sent to environment entities\n        # and some of them may be used to control various entities in the environment\n        # e.g. motion of obstacles, etc.\n        env_spacing = 1.0  # not used with heightfields/trimeshes\n        num_physics_steps_per_env_step_mean = 1  # number of steps between camera renders mean\n        num_physics_steps_per_env_step_std = 0  # number of steps between camera renders std\n        render_viewer_every_n_steps = 10  # render the viewer every n steps\n        collision_force_threshold = 0.010  # collision force threshold\n        manual_camera_trigger = False  # trigger camera captures manually\n        reset_on_collision = (\n            True  # reset environment when contact force on quadrotor is above a threshold\n        )\n        create_ground_plane = False  # create a ground plane\n        sample_timestep_for_latency = True  # sample the timestep for the latency noise\n        perturb_observations = True\n        keep_same_env_for_num_episodes = 1\n\n        use_warp = False\n        e_s = env_spacing\n        lower_bound_min = [-e_s, -e_s, -e_s]  # lower bound for the environment space\n        lower_bound_max = [-e_s, -e_s, -e_s]  # lower bound for the environment space\n        upper_bound_min = [e_s, e_s, e_s]  # upper bound for the environment space\n        upper_bound_max = [e_s, e_s, e_s]  # upper bound for the environment space\n\n    class env_config:\n        include_asset_type = {}\n\n        asset_type_to_dict_map = {}\n</code></pre> <p>In order to add assets to the environment, the <code>include_asset_type</code> dictionary can be used to specify the assets that are to be included in the environment. The <code>asset_type_to_dict_map</code> dictionary maps the asset type to the class defining the asset parameters.</p> Environment configuration file for an environment with obstacles <p>for the case of an environment with obstacles can be seen below: <pre><code>from aerial_gym.config.env_config.env_object_config import EnvObjectConfig\n\nimport numpy as np\n\n\nclass EnvWithObstaclesCfg(EnvObjectConfig):\n    class env:\n        num_envs = 64\n        num_env_actions = 4  # this is the number of actions handled by the environment\n        # potentially some of these can be input from the RL agent for the robot and\n        # some of them can be used to control various entities in the environment\n        # e.g. motion of obstacles, etc.\n        env_spacing = 5.0  # not used with heightfields/trimeshes\n\n        num_physics_steps_per_env_step_mean = 10  # number of steps between camera renders mean\n        num_physics_steps_per_env_step_std = 0  # number of steps between camera renders std\n\n        render_viewer_every_n_steps = 1  # render the viewer every n steps\n        reset_on_collision = (\n            True  # reset environment when contact force on quadrotor is above a threshold\n        )\n        collision_force_threshold = 0.05  # collision force threshold [N]\n        create_ground_plane = False  # create a ground plane\n        sample_timestep_for_latency = True  # sample the timestep for the latency noise\n        perturb_observations = True\n        keep_same_env_for_num_episodes = 1\n\n        use_warp = True\n        lower_bound_min = [-2.0, -4.0, -3.0]  # lower bound for the environment space\n        lower_bound_max = [-1.0, -2.5, -2.0]  # lower bound for the environment space\n        upper_bound_min = [9.0, 2.5, 2.0]  # upper bound for the environment space\n        upper_bound_max = [10.0, 4.0, 3.0]  # upper bound for the environment space\n\n    class env_config:\n        include_asset_type = {\n            \"panels\": True,\n            \"thin\": False,\n            \"trees\": False,\n            \"objects\": True,\n            \"left_wall\": True,\n            \"right_wall\": True,\n            \"back_wall\": True,\n            \"front_wall\": True,\n            \"top_wall\": True,\n            \"bottom_wall\": True,\n        }\n\n        # maps the above names to the classes defining the assets. They can be enabled and disabled above in include_asset_type\n        asset_type_to_dict_map = {\n            \"panels\": EnvObjectConfig.panel_asset_params,\n            \"thin\": EnvObjectConfig.thin_asset_params,\n            \"trees\": EnvObjectConfig.tree_asset_params,\n            \"objects\": EnvObjectConfig.object_asset_params,\n            \"left_wall\": EnvObjectConfig.left_wall,\n            \"right_wall\": EnvObjectConfig.right_wall,\n            \"back_wall\": EnvObjectConfig.back_wall,\n            \"front_wall\": EnvObjectConfig.front_wall,\n            \"bottom_wall\": EnvObjectConfig.bottom_wall,\n            \"top_wall\": EnvObjectConfig.top_wall,\n        }\n</code></pre></p>"},{"location":"4_simulation_components/#tasks","title":"Tasks","text":"<p>An environment specification determines what is populated in an independent simulation instance and how the collective simulation steps through time based on commanded actions. The task here is however slightly different. We intend to use this term of interpreting task-specific information from the environment. A task class instantiates the entire simulation with all its parallel robots and assets and therefore, has access to all the simulation information. We intend to use this class to determine how the environment is interpreted for RL tasks. For example, a given simulation instance with sim params, environment and asset specification, robot, sensors and controller specifications, can be utilized to train a policy to perform completely different tasks. Example of these could include setpoint navigation through clutter, observing a specific asset in the simulation, perching on that specific asset in simulation and so on and so forth. All these tasks can be performed with the same set of objects in the environment, but require a different interpretation of the environment data for training the RL algorithm appropriately. This can be done within the Task classes. A task can be specified in the files in the <code>config/task_config</code> folder as:</p> Example of a Task Configuration <pre><code>class task_config:\n    seed = 10\n    sim_name = \"base_sim\"\n    env_name = \"empty_env\"\n    robot_name = \"base_quadrotor\"\n    args = {}\n    num_envs = 2\n    device = \"cuda:0\"\n    observation_space_dim = 13\n    privileged_observation_space_dim = 0\n    action_space_dim = 4\n    episode_len_steps = 1000 # real physics time for simulation is this value multiplied by sim.dt\n    return_state_before_reset = False\n    reward_parameters = {\n        \"pos_error_gain1\": [2.0, 2.0, 2.0],\n        \"pos_error_exp1\": [1/3.5, 1/3.5, 1/3.5],\n        \"pos_error_gain2\": [2.0, 2.0, 2.0],\n        \"pos_error_exp2\": [2.0, 2.0, 2.0],\n        \"dist_reward_coefficient\": 7.5,\n        \"max_dist\": 15.0,\n        \"action_diff_penalty_gain\": [1.0, 1.0, 1.0],\n        \"absolute_action_reward_gain\": [2.0, 2.0, 2.0],\n        \"crash_penalty\": -100,\n    }\n\n    # a + bx for action scaling\n    consant_for_action = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32, device=device)\n\n    scale_for_action = torch.tensor([3.0, 3.0, 3.0, 1.50], dtype=torch.float32, device=device)\n\n\n    def action_transformation_function(action):\n        clamped_action = torch.clamp(action, -1.0, 1.0)\n        return task_config.consant_for_action + task_config.scale_for_action * clamped_action     \n</code></pre> <p>A sample task for position setpoint navigation (without sensors or obstacles) is provided as an example:</p> <code>PositionSetpointTask</code> class definition example <pre><code>class PositionSetpointTask(BaseTask):\n    def __init__(self, task_config):\n        super().__init__(task_config)\n        self.device = self.task_config.device\n        # set the each of the elements of reward parameter to a torch tensor\n        # common boilerplate code here\n\n        # Currently only the \"observations\" are sent to the actor and critic.\n        # The \"privileged_obs\" are not handled so far in sample-factory\n\n        self.task_obs = {\n            \"observations\": torch.zeros(\n                (self.sim_env.num_envs, self.task_config.observation_space_dim),\n                device=self.device,\n                requires_grad=False,\n            ),\n            \"priviliged_obs\": torch.zeros(\n                (self.sim_env.num_envs, self.task_config.privileged_observation_space_dim),\n                device=self.device,\n                requires_grad=False,\n            ),\n            \"collisions\": torch.zeros(\n                (self.sim_env.num_envs, 1), device=self.device, requires_grad=False\n            ),\n            \"rewards\": torch.zeros(\n                (self.sim_env.num_envs, 1), device=self.device, requires_grad=False\n            ),\n        }\n\n    # common boilerplate code here\n\n    def step(self, actions):\n        # this uses the action, gets observations\n        # calculates rewards, returns tuples\n        # In this case, the episodes that are terminated need to be\n        # first reset, and the first observation of the new episode\n        # needs to be returned.\n\n        transformed_action = self.action_transformation_function(actions)\n        self.sim_env.step(actions=transformed_action)\n\n        # This step must be done since the reset is done after the reward is calculated.\n        # This enables the robot to send back an updated state, and an updated observation to the RL agent after the reset.\n        # This is important for the RL agent to get the correct state after the reset.\n        self.rewards[:], self.terminations[:] = self.compute_rewards_and_crashes(self.obs_dict)\n\n        if self.task_config.return_state_before_reset == True:\n            return_tuple = self.get_return_tuple()\n\n\n        self.truncations[:] = torch.where(self.sim_env.sim_steps &gt; self.task_config.episode_len_steps, 1, 0)\n        self.sim_env.post_reward_calculation_step()\n\n        self.infos = {}  # self.obs_dict[\"infos\"]\n\n        if self.task_config.return_state_before_reset == False:\n            return_tuple = self.get_return_tuple()\n        return return_tuple\n\n    ...\n\n    def process_obs_for_task(self):\n        self.task_obs[\"observations\"][:, 0:3] = (\n            self.target_position - self.obs_dict[\"robot_position\"] # position in environment/world frame\n        )\n        self.task_obs[\"observations\"][:, 3:7] = self.obs_dict[\"robot_orientation\"] # orientation in environment/world frame\n        self.task_obs[\"observations\"][:, 7:10] = self.obs_dict[\"robot_body_linvel\"] # linear velocity in the body/imu frame\n        self.task_obs[\"observations\"][:, 10:13] = self.obs_dict[\"robot_body_angvel\"] # angular velocity in the body/imu frame\n        self.task_obs[\"rewards\"] = self.rewards # reward for the time step after it is calculated\n        self.task_obs[\"terminations\"] = self.terminations # terminations/crashes \n        self.task_obs[\"truncations\"] = self.truncations # truncations or premature resets (for purposes of diversifying data and episodes)\n\n\n@torch.jit.script\ndef exp_func(x, gain, exp):\n    return gain * torch.exp(-exp * x * x)\n\n\n@torch.jit.script\ndef compute_reward(\n    pos_error, crashes, action, prev_action, curriculum_level_multiplier, parameter_dict\n):\n    # type: (Tensor, Tensor, Tensor, Tensor, float, Dict[str, Tensor]) -&gt; Tuple[Tensor, Tensor]\n    dist = torch.norm(pos_error, dim=1)\n\n    pos_reward = 2.0 / ( 1.0 + dist * dist)\n\n    dist_reward = (20 - dist) / 20.0\n\n    total_reward = (\n        pos_reward + dist_reward # + up_reward + action_diff_reward + absolute_action_reward\n    )\n    total_reward[:] = curriculum_level_multiplier * total_reward\n    crashes[:] = torch.where(dist &gt; 8.0, torch.ones_like(crashes), crashes)\n\n    total_reward[:] = torch.where(\n        crashes &gt; 0.0, -2 * torch.ones_like(total_reward), total_reward\n    )\n    return total_reward, crashes\n</code></pre> <p>The Task class is ultimately designed to be used with RL frameworks, therefore conforms to the Gymnasium API specification. In this above class, the <code>step(...)</code> function first translates the commands from the RL agent to a control command for the specific robot platform by transforming the action input. Subsequently this is commanded to the robot and the environment is stepped. Finally, a reward is computed for the new state and truncations and terminations are determined and the final tuple is returned for use by the RL framework. Similarly, for trajectory tracking, only the reward function and observation needs to be changed to train the RL algorithm without making any changes to the asset, robots or the environment.</p> <p>To add your own custom tasks please refer to the section on customizing the simulator.</p> Difference between Environment and Task <p>A lot of different simulator implementations interchange the terminologies. In our case, we view the environment as the components that define the robot, it's physics surroundings, i.e., the assets near the robot, the parameters of the physics engine that detemine how the various entities in the simulation world interact with one another, and how sensors perceive the data via the sensor parameters.</p> <p>The task on the other hand is an interpretation of the simulation world and the information provided by / collected from it to reach a particular goal that is desired by the user. The same environment can be used to train multiple tasks and the tasks can be changed without changing an environment definition.</p> <p>For example, an empty environment with a quadrotor can be used to train a position setpoint task, or a trajectory tracking task. An environment with a set of obstacles can be used to train a policy that can either navigate through the obstacles or perch on a specific asset in the environment. The task is the interpretation of the environment data for the RL algorithm to learn the desired behavior.</p> <p>To relate it with a familiar environment from the OpenAI Gym suite of tasks, an \"environment\" in our case could refer to a CartPole world with its associated dynamics, however a \"task\" in our case would allow the same cartpole to be controlled to balance the pole upright, or to keep swinging the pole at a given angular rate or to have the endpoint of the pole at a given location in the environment. All of which require different formulation of rewards and observations for the RL algorithm to learn the desired behavior.</p>"},{"location":"4_simulation_components/#robots","title":"Robots","text":"<p>Robots can be specified and configured independently of the sensors and the environment using a robot registry. More about robots can be found on the page for Robots and Controllers</p>"},{"location":"4_simulation_components/#controllers","title":"Controllers","text":"<p>Controllers can be specified and selected independently of the robot platform. Note however, that all combinations of controllers will not produce optimal results with all platforms. The controllers can be registered and selected from the controller registry. More about controllers can be found on the page for Robots and Controllers.</p>"},{"location":"4_simulation_components/#sensors","title":"Sensors","text":"<p>Similar to the above, the sensors can be specified and selected independently. However since the sensors are mounted on the robot platform, we have made a choice to select the sensors for the robot in the robot config file, and not directly as a registry (it is possible to do so yourself with very minor changes to the code). More about the capabilities of the sensors can be found on the page for Sensors and Rendering.</p>"},{"location":"5_customization/","title":"Customizing the Simulator","text":""},{"location":"5_customization/#custom-physics-parameters","title":"Custom Physics Parameters","text":"<p>Different physics engine + viewer window combination parameters can be set in the <code>sim</code> class with another class name. This configuration can be registered with to be used in the simulation either in the <code>__init__.py</code> file in the relevant folder, or at runtime in the code by the following command:</p> Custom simulation parameters <pre><code>from aerial_gym.registry.sim_registry import sim_registry\nfrom aerial_gym.simulation.sim_params import BaseSimParams\n\nclass CustomSimParamsFallingForwards(BaseSimParams):\n    class sim(BaseSimConfig.sim):\n        dt = 0.01  # Custom Parameter\n        gravity = [+1.0, 0.0, 0.0]\n\n\n# register your custom class here\nsim_registry.register_sim_params(\"falling_forwards\", CustomSimParamsFallingForwards)\n\n### use the registered class further in the code to spawn a simulation ###\n</code></pre> <p>A config file can then be created that specifies that the simulation should use the custom parameters. The simulator (Isaac Gym instance within the code) will have to be restarted for the changes to take effect.</p> <p>The physics simulation parameters can be registered via the sim_registry</p> <ul> <li>in the file <code>sim/__init__.py</code> for it to be named and identifiable throughout the simulation run, or</li> <li>at runtime in the code by the following command:     <pre><code>from aerial_gym.registry.sim_registry import sim_registry\nsim_registry.register_sim_params(\"falling_forwards_sim_params\", CustomSimParamsFallingForwards)\n</code></pre></li> </ul> <p>The simulation instance needs to be restarted for the parameters to take effect</p>"},{"location":"5_customization/#custom-environments","title":"Custom Environments","text":"<p>We provide an example environment that has milti-linked parametric tree objects in the environment to simulate a forest.</p> <p>More objects can be added to the environment by setting their asset properties and creating an environment file for that environment that specifies which assets are to be included in the environment.</p> <p>In this manner, it is:</p> <ol> <li>Possible to reuse the same (set of) assets across environments</li> <li>Easy to compose multiple environments with same or varying assets with different parameters for randomization.</li> <li>Include your own assets and create custom environments for specific tasks.</li> </ol> The configuration file for the forest environment <pre><code>from aerial_gym.config.env_config.env_object_config import EnvObjectConfig\n\nimport numpy as np\n\n\nclass ForestEnvCfg(EnvObjectConfig):\n    class env:\n        num_envs = 64\n        num_env_actions = 4\n        env_spacing = 5.0  # not used with heightfields/trimeshes\n        num_physics_steps_per_env_step_mean = 10  # number of steps between camera renders mean\n        num_physics_steps_per_env_step_std = 0  # number of steps between camera renders std\n        render_viewer_every_n_steps = 1  # render the viewer every n steps\n        reset_on_collision = (\n            True  # reset environment when contact force on quadrotor is above a threshold\n        )\n        collision_force_threshold = 0.005  # collision force threshold [N]\n        create_ground_plane = False  # create a ground plane\n        sample_timestep_for_latency = True  # sample the timestep for the latency noise\n        perturb_observations = True\n        keep_same_env_for_num_episodes = 1\n        use_warp = True\n        lower_bound_min = [-5.0, -5.0, -1.0]  # lower bound for the environment space\n        lower_bound_max = [-5.0, -5.0, -1.0]  # lower bound for the environment space\n        upper_bound_min = [5.0, 5.0, 3.0]  # upper bound for the environment space\n        upper_bound_max = [5.0, 5.0, 3.0]  # upper bound for the environment space\n\n    class env_config:\n        include_asset_type = {\n            \"trees\": True,\n            \"objects\": True,\n            \"bottom_wall\": True,\n        }\n\n        # maps the above names to the classes defining the assets. They can be enabled and disabled above in include_asset_type\n        asset_type_to_dict_map = {\n            \"trees\": EnvObjectConfig.tree_asset_params,\n            \"objects\": EnvObjectConfig.object_asset_params,\n            \"bottom_wall\": EnvObjectConfig.bottom_wall,\n        }\n</code></pre> <p>The environment appears as shown below:</p> <p></p>"},{"location":"5_customization/#custom-controllers","title":"Custom Controllers","text":"<p>Additional controllers can be added as per the need of the users for their preferred robot configuration. We provide an example of a non-standard controller that tracks a velocity and steering angle command in the <code>controller</code> folder. The vehicle velocity is expressed in the vehicle frame and the steering angle is measured w.r.t the world frame. The controller can be registered in the <code>__init__.py</code> file in the <code>controller</code> folder, or at runtime in the code. To better show the integration with our existing code, we exploit the functionality provided by the <code>base_lee_controller.py</code> class, however the users are not required to adhere to this and can write their own controller structures as per their requirements. We also modify the controller from the <code>base_lee_controller.py</code> file to show the control of a fully actuated platform with 8 motors and use it to control a model of an underwater vehicle. We also provide an example file to simulate this controller with an underwater robot model.</p> <code>FullyActuatedController</code> code <pre><code>class FullyActuatedController(BaseLeeController):\n    def __init__(self, config, num_envs, device):\n        super().__init__(config, num_envs, device)\n\n    def init_tensors(self, global_tensor_dict=None):\n        super().init_tensors(global_tensor_dict)\n\n    def update(self, command_actions):\n        \"\"\"\n        Fully actuated controller. Input is in the form of desired position and orientation.\n        command_actions = [p_x, p_y, p_z, qx, qy, qz, qw]\n        Position setpoint is in the world frame\n        Orientation reference is w.r.t world frame\n        \"\"\"\n        self.reset_commands()\n        command_actions[:, 3:7] = normalize(command_actions[:, 3:7])\n        self.accel[:] = self.compute_acceleration(command_actions[:, 0:3], torch.zeros_like(command_actions[:, 0:3]))\n        forces = self.mass * (self.accel - self.gravity)\n        self.wrench_command[:, 0:3] = quat_rotate_inverse(\n            self.robot_orientation, forces\n        )\n        self.desired_quat[:] = command_actions[:, 3:]\n        self.wrench_command[:, 3:6] = self.compute_body_torque(\n            self.desired_quat, torch.zeros_like(command_actions[:, 0:3])\n        )\n        return self.wrench_command\n</code></pre> <p>The controller can be registered via the controller_registry</p> <ul> <li>in the file <code>controller/__init__.py</code> for it to be named and identifiable throughout the simulation run, or</li> <li>at runtime in the code by the following command:     <pre><code>from aerial_gym.registry.controller_registry import controller_registry\ncontroller_registry.register_controller(\n    \"fully_actuated_control\", FullyActuatedController, fully_actuated_controller_config\n)\n</code></pre></li> </ul>"},{"location":"5_customization/#custom-robots","title":"Custom Robots","text":"<p>We support addition of custom robot configurations and custom control methodologies in the simulator. An example of an arbitrary robot configuration with 8 motors is provided with the simulator. The robot configuration, if differing significantly, can have its own python class to control the robot links, coontrollers, and utilize the sensors that are onboard the robot. The robot configuration can be registered in the <code>__init__.py</code> file in the <code>robot</code> folder, or at runtime in the code. Moreover, the current file structure allows for us to reuse the same robot class with an appropriate configuration file.</p> <p>In our case, we use the <code>base_quadrotor.py</code> class alongside with an appropriate configuration file for the robot. For example, for the arbitrary robot model, we use the following configuration file:</p> Configuration file for arbitrary robot model <pre><code># asset parameters for the simulator above\n\n\nclass control_allocator_config:\n    num_motors = 8\n    force_application_level = \"motor_link\"\n    # \"motor_link\" or \"root_link\" to apply forces at the root link or at the individual motor links\n\n    motor_mask = [1 + 8 + i for i in range(0, 8)]\n    motor_directions = [1, -1, 1, -1, 1, -1, 1, -1]\n\n    allocation_matrix = [[ 5.55111512e-17, -3.21393805e-01, -4.54519478e-01, -3.42020143e-01,\n                        9.69846310e-01,  3.42020143e-01,  8.66025404e-01, -7.54406507e-01],\n                        [ 1.00000000e+00, -3.42020143e-01, -7.07106781e-01,  0.00000000e+00,\n                        -1.73648178e-01,  9.39692621e-01,  5.00000000e-01, -1.73648178e-01],\n                        [ 1.66533454e-16, -8.83022222e-01,  5.41675220e-01,  9.39692621e-01,\n                        1.71010072e-01,  1.11022302e-16,  1.11022302e-16,  6.33022222e-01],\n                        [ 1.75000000e-01,  1.23788742e-01, -5.69783368e-02,  1.34977168e-01,\n                        3.36959042e-02, -2.66534135e-01, -7.88397460e-02, -2.06893989e-02],\n                        [ 1.00000000e-02,  2.78845133e-01, -4.32852308e-02, -2.72061766e-01,\n                        -1.97793856e-01,  8.63687139e-02,  1.56554446e-01, -1.71261290e-01],\n                        [ 2.82487373e-01, -1.41735490e-01, -8.58541103e-02,  3.84858939e-02,\n                        -3.33468026e-01,  8.36741468e-02,  8.46777988e-03, -8.74336259e-02]]\n\n    # here, the allocation matrix is computed (by the user) to from the URDF files of the robot\n    # to map the effect of motor forces on the net force and torque acting on the robot.\n\nclass motor_model_config:\n    motor_time_constant_min = 0.01\n    motor_time_constant_max = 0.03\n    max_thrust = 5.0\n    min_thrust = -5.0\n    max_thrust_rate = 100.0\n    thrust_to_torque_ratio = 0.01 # thrust to torque ratio is related to inertia matrix dont change\n\n# other parameters for the robot below\n</code></pre> <p>Additionally, we also provide an example of control of an underwater BlueROV robot model with 8 motors and a custom controller for the fully-actutated platform. We provide an example file showing full-state tracking of the robot with the controller.</p> <p></p>"},{"location":"5_customization/#custom-tasks","title":"Custom Tasks","text":"<p>You can refer to the example file in <code>tasks/custom_task</code> and implement your own task specification as shown here:</p> Custom Task Class Definition <pre><code>class CustomTask(BaseTask):\n    def __init__(self, task_config):\n        super().__init__(task_config)\n        self.device = self.task_config.device\n        # write your own implementation herer\n\n        self.sim_env = SimBuilder().build_env(\n            sim_name=self.task_config.sim_name,\n            env_name=self.task_config.env_name,\n            robot_name=self.task_config.robot_name,\n            args=self.task_config.args,\n            device=self.device,\n        )\n\n        # Implement something here that is relevant to your task\n\n        self.task_obs = {\n            \"observations\": torch.zeros(\n                (self.sim_env.num_envs, self.task_config.observation_space_dim),\n                device=self.device,\n                requires_grad=False,\n            ),\n            \"priviliged_obs\": torch.zeros(\n                (self.sim_env.num_envs, self.task_config.privileged_observation_space_dim),\n                device=self.device,\n                requires_grad=False,\n            ),\n            \"collisions\": torch.zeros(\n                (self.sim_env.num_envs, 1), device=self.device, requires_grad=False\n            ),\n            \"rewards\": torch.zeros(\n                (self.sim_env.num_envs, 1), device=self.device, requires_grad=False\n            ),\n        }\n\n    def close(self):\n        self.sim_env.delete_env()\n\n    def reset(self):\n        # write your implementation here\n        return None\n\n    def reset_idx(self, env_ids):\n        # write your implementation here\n        return\n\n    def render(self):\n        return self.sim_env.render()\n\n    def step(self, actions):\n        # this uses the action, gets observations\n        # calculates rewards, returns tuples\n        # In this case, the episodes that are terminated need to be\n        # first reset, and the first obseration of the new episode\n        # needs to be returned.\n\n        # repace this with something that is relevant to your task\n        self.sim_env.step(actions=actions)\n\n\n        return None # replace this with something relevant to your task\n\n@torch.jit.script\ndef compute_reward(\n    pos_error, crashes, action, prev_action, curriculum_level_multiplier, parameter_dict\n):\n    # something here\n    return 0\n</code></pre> <p>The task must be registered via the task-registry </p> <ul> <li>in the file <code>task/__init__.py</code> for it to be named and identifiable throughout the simulation run, or</li> <li>at runtime in the code by the following command:     <pre><code>from aerial_gym.registry.task_registry import task_registry\ntask_registry.register_task(\"custom_task\", CustomTask)\n</code></pre></li> </ul>"},{"location":"5_customization/#custom-sensors","title":"Custom Sensors","text":"<p>Exposing sensor parameters for the ray-casting sensor allows for them to be individually customized to simulate an exteroceptive sensor by the user. We provide an example of a hemispherical LiDAR sensor based on Ouster OSDome LiDAR.</p> The parameters can be configured as shown here <pre><code>from aerial_gym.config.sensor_config.lidar_config.base_lidar_config import BaseLidarConfig\n\nclass OSDome_64_Config(BaseLidarConfig):\n    # keep everything pretty much the same and change the number of vertical rays\n    height = 64\n    width = 512\n    horizontal_fov_deg_min = -180\n    horizontal_fov_deg_max = 180\n    vertical_fov_deg_min = 0\n    vertical_fov_deg_max = 90\n    max_range = 20.0\n    min_range = 0.5\n\n    return_pointcloud= False\n    segmentation_camera = True\n\n    # randomize placement of the sensor\n    randomize_placement = False\n    min_translation = [0.0, 0.0, 0.0]\n    max_translation = [0.0, 0.0, 0.0]\n    # example of a front-mounted dome lidar\n    min_euler_rotation_deg = [0.0, 0.0, 0.0]\n    max_euler_rotation_deg = [0.0, 0.0, 0.0]\n    class sensor_noise:\n        enable_sensor_noise = False\n        pixel_dropout_prob = 0.01\n        pixel_std_dev_multiplier = 0.01\n</code></pre> <p>The data from the hemispherical LiDAR is projected into a range and segmented image and appears as shown below:</p> <p> </p>"},{"location":"6_rl_training/","title":"Reinforcement Learning","text":""},{"location":"6_rl_training/#reinforcement-learning-for-navigation-tasks-using-depth-images","title":"Reinforcement Learning for Navigation Tasks using Depth Images","text":"<p>We provide a ready-to-use policy that was used for the work in Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding. The observation space is redefined to match the example shown in the paper and a script is provided to run the inference on the trained policy. To check out the performance of the policy yourself, please follow the steps below:</p> <pre><code>cd examples/dce_rl_navigation\nbash run_trained_navigation_policy.sh\n</code></pre> <p>You should now be able to see the trained policy in action: </p> <p>For this task, the rendering is done using Warp by default and the robot's depth camera sees the environment as shown below:</p> <p> </p> <p>If you use this work, please cite the following paper:</p> <pre><code>@misc{kulkarni2024reinforcementlearningcollisionfreeflight,\n      title={Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding}, \n      author={Mihir Kulkarni and Kostas Alexis},\n      year={2024},\n      eprint={2402.03947},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO},\n      url={https://arxiv.org/abs/2402.03947}, \n}\n</code></pre>"},{"location":"6_rl_training/#train-policies-for-your-own-robot-with-the-aerial-gym-simulator","title":"Train policies for your own robot with the Aerial Gym Simulator","text":"<p>We provide examples with rl-games, sample-factory and CleanRL frameworks out-of-the-box with relevant scripts for the same. The <code>Task</code> definition of the simulator allows for a minimalistic integration possible with the simulator allowing the developed/user to focus on designing an appropriate simulation environment rather than spending time on integrating the environment with the RL training framework.</p>"},{"location":"6_rl_training/#train-your-own-navigation-policies","title":"Train your own navigation policies","text":"<p>Similar task-configuration can be done to enable navigation control policies, however the robot needs to simulate the exteroceptive sensor data using the camera sensor onboard. This change occurs on the robot side and requires enabling the cameras / LiDAR sensors on the robot.</p> <p>The <code>config/robot_config/base_quad_config.py</code> or corresponding robot configuration files require modification to enable the camera sensor. The camera sensor can be enabled as follows: <pre><code>class BaseQuadCfg:\n    ...\n    class sensor_config:\n        enable_camera = True # False when you do not need a camera sensor\n        camera_config = BaseDepthCameraConfig\n\n        enable_lidar = False\n        lidar_config = BaseLidarConfig  # OSDome_64_Config\n\n        enable_imu = False\n        imu_config = BaseImuConfig\n    ...\n</code></pre></p> <p>Subsequently, the various algorithms can be trained using the provided training scripts in the <code>rl_training</code> folder.</p> <p>Example with <code>rl_games</code>: <pre><code>### Train the navigation policy for the quadrotor with velocity control\npython3 runner.py --file=./ppo_aerial_quad_navigation.yaml --num_envs=1024 --headless=True\n\n### Replay the trained policy in simulation\npython3 runner.py --file=./ppo_aerial_quad_navigation.yaml --num_envs=16 --play --checkpoint=./runs/&lt;weights_file_path&gt;/&lt;weights_filename&gt;.pth\n</code></pre></p> <p>The training takes approximately an hour on a single NVIDIA RTX 3090 GPU. The <code>navigation_task</code> uses a Deep Collision Encoder by default to represent the latent representation considering obstacles inflated by the robot size.</p>"},{"location":"6_rl_training/#train-your-own-position-control-policies","title":"Train your own position-control policies","text":"<p>We provide readymade task-definitions to train policies for position control of various robots. The task definition remains same - regardless of the configuration and the reward function may be modified to suit the user's needs for specific requirements in performance such as smoothness in control, energy efficiency etc. The provided RL algorithms will learn to generate control commands for the onboard controller or provide direct motor commands to the robots.</p> <p>For training position-setpoint policies for various robots using various onboard controllers, configure the <code>config/task_config/position_setpoint_task_config.py</code> as follows: <pre><code>class task_config:\n    seed = -1 # set your seed here. -1 will randomize the seed\n    sim_name = \"base_sim\"\n\n    env_name = \"empty_env\"\n    # env_name = \"env_with_obstacles\"\n    # env_name = \"forest_env\"\n\n    robot_name = \"base_quadrotor\"\n    # robot_name = \"base_fully_actuated\"\n    # robot_name = \"base_random\"\n\n\n    controller_name = \"lee_attitude_control\"\n    # controller_name = \"lee_acceleration_control\"\n    # controller_name = \"no-control\"\n    ...\n</code></pre></p> <p>To train a policy with <code>rl_games</code>, please run the following command: <pre><code># train the policy\npython3 runner.py --task=position_setpoint_task --num_envs=8192 --headless=True --use_warp=True\n\n# replay the trained policy\npython3 runner.py --task=position_setpoint_task --num_envs=16 --headless=False --use_warp=True --checkpoint=&lt;path_to_checkpoint_weights&gt;.pth --play\n</code></pre></p> <p>This policy trains in under a minute using a single NVIDIA RTX 3090 GPU. The resultant trained policy is shown below:</p> <p></p>"},{"location":"6_rl_training/#rl-games","title":"rl-games","text":"<p>The <code>Task</code> instance of the simulation is required to be wrapped with a framework-specific environment wrapper for parallel environments.</p> Example rl-games Wrapper <pre><code>class ExtractObsWrapper(gym.Wrapper):\n    def __init__(self, env):\n        super().__init__(env)\n\n    def reset(self, **kwargs):\n        observations, *_ = super().reset(**kwargs)\n        return observations[\"observations\"]\n\n    def step(self, action):\n        observations, rewards, terminated, truncated, infos = super().step(\n            action\n        )\n\n        dones = torch.where(terminated | truncated, torch.ones_like(terminated), torch.zeros_like(terminated))\n\n        return (\n            observations[\"observations\"],\n            rewards,\n            dones,\n            infos,\n        )\n\n\nclass AERIALRLGPUEnv(vecenv.IVecEnv):\n    def __init__(self, config_name, num_actors, **kwargs):\n        print(\"AERIALRLGPUEnv\", config_name, num_actors, kwargs)\n        print(env_configurations.configurations)\n        self.env = env_configurations.configurations[config_name][\n            \"env_creator\"\n        ](**kwargs)\n\n        self.env = ExtractObsWrapper(self.env)\n\n    def step(self, actions):\n        return self.env.step(actions)\n\n    def reset(self):\n        return self.env.reset()\n\n    def reset_done(self):\n        return self.env.reset_done()\n\n    def get_number_of_agents(self):\n        return self.env.get_number_of_agents()\n\n    def get_env_info(self):\n        info = {}\n        info[\"action_space\"] = spaces.Box(\n            np.ones(self.env.task_config.action_space_dim) * -1.0, np.ones(self.env.task_config.action_space_dim) * 1.0\n        )\n        info[\"observation_space\"] = spaces.Box(\n            np.ones(self.env.task_config.observation_space_dim) * -np.Inf, np.ones(self.env.task_config.observation_space_dim) * np.Inf\n        )\n\n        print(info[\"action_space\"], info[\"observation_space\"])\n\n        return info\n</code></pre> <p>Here the environment is wrapper inside an <code>AERIALRLGPUEnv</code> instance. The <code>ExtractObsWrapper</code> is a gym wrapper that allows to extract the observations from the environment. While this is not necessary given that our <code>Task</code>  allows this flexibility within it, we have retained this structure to maintain consistency with our previous release and other implementations.</p> <p>An example of the rl-games training wrapper for the position setpoint task with attitude control for a quadrotor is shown below:</p> <p></p> <p>Similarly, an example for the position setpoint task using motor commands for a fully-actutaed octarotor and a random configuration with 8 motors is shown below:</p> <p></p> <p></p>"},{"location":"6_rl_training/#sample-factory","title":"Sample Factory","text":"<p>Similar to the description above, Sample Factory integration requries a gym Wrapper. This is also done as follows:</p> Example Sample Factory Wrapper <pre><code>class AerialGymVecEnv(gym.Env):\n    '''\n    Wrapper for Aerial Gym environments to make them compatible with the sample factory.\n    '''\n    def __init__(self, aerialgym_env, obs_key):\n        self.env = aerialgym_env\n        self.num_agents = self.env.num_envs\n        self.action_space = convert_space(self.env.action_space)\n\n        # Aerial Gym examples environments actually return dicts\n        if obs_key == \"obs\":\n            self.observation_space = gym.spaces.Dict(convert_space(self.env.observation_space))\n        else:\n            raise ValueError(f\"Unknown observation key: {obs_key}\")\n\n        self._truncated: Tensor = torch.zeros(self.num_agents, dtype=torch.bool)\n\n    def reset(self, *args, **kwargs) -&gt; Tuple[Dict[str, Tensor], Dict]:\n        # some IGE envs return all zeros on the first timestep, but this is probably okay\n        obs, rew, terminated, truncated, infos = self.env.reset()\n        return obs, infos\n\n    def step(self, action) -&gt; Tuple[Dict[str, Tensor], Tensor, Tensor, Tensor, Dict]:\n        obs, rew, terminated, truncated, infos = self.env.step(action)\n        return obs, rew, terminated, truncated, infos\n\n    def render(self):\n        pass\n\n\ndef make_aerialgym_env(full_task_name: str, cfg: Config, _env_config=None, render_mode: Optional[str] = None) -&gt; Env:\n\n    return AerialGymVecEnv(task_registry.make_task(task_name=full_task_name), \"obs\")\n</code></pre> <p>An example of the position setpoint task with attitude control for a quadrotor is shown below:</p> <p></p>"},{"location":"6_rl_training/#cleanrl","title":"CleanRL","text":"<p>The flexibility provided by the task definition for RL allows it to be directly used with the CleanRL framework with no changes:</p> Example for CleanRL Wrapper <pre><code>    # env setup\n    envs = task_registry.make_task(task_name=args.task)\n\n    envs = RecordEpisodeStatisticsTorch(envs, device)\n\n    print(\"num actions: \", envs.task_config.action_space_dim)\n    print(\"num obs: \", envs.task_config.observation_space_dim)\n</code></pre> <p>An example of the position setpoint task with attitude control for a quadrotor is shown below:</p> <p></p>"},{"location":"6_rl_training/#adding-your-own-rl-frameworks","title":"Adding your own RL frameworks","text":"<p>Kindly refer to the existing implementations for sample-factory, rl-games and CleanRL for reference. We would love to include your implementations within the simulator allowing it to be more accessible to more users with different needs/preferences. If you wish to contribute to this repository, please open a Pull Request on GitHub. </p>"},{"location":"7_FAQ_and_troubleshooting/","title":"FAQs and Troubleshooting","text":""},{"location":"7_FAQ_and_troubleshooting/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>When is the support for Isaac Lab expected?</p> <p>We are working on supporting Isaac Lab in the near future. Please stay tuned for updates on this.</p> <p>How can I use the Isaac Gym Simulator with my custom robot?</p> <p>The Isaac Gym Simulator is designed to be modular and flexible, allowing users to easily integrate their custom robots. You can refer to the Custom Robot Integration section of the documentation for detailed instructions on how to integrate your custom robot with the Isaac Gym Simulator.</p> <p>How can I randomize the pose of the sensor on the robot</p> <p>The pose of the sensor can be randomized by enabling the <code>randomize_placement</code> flag in the sensor configuration file. This is however only applicable to Warp rendering pipeline and is very slow with the Isaac Gym's native rendering pipeline as it requires the user to loop through each sensor instance. By default the sensor position is randomized at each reset of the environment, however you can randomize at each timestep if you like with little overhead.</p> <p>How can I change the randomize the pose at which the robot spawns</p> <p>This can be set using the <code>min_init_state</code> and the <code>max_init_state</code> parameters in the robot configuration file. The starting pose of the robot is randomized at each reset of the environment by default. Based on the current structure, the position is a ratio of the environment bounds, and the orientation can be defined by minimum and maximum roll, pitch and yaw values.</p> <p>Difference between Environment and Task</p> <p>A lot of different simulator implementations interchange the terminologies. In our case, we view the environment as the components that define the robot, it's physics surroundings, i.e., the assets near the robot, the parameters of the physics engine that detemine how the various entities in the simulation world interact with one another, and how sensors perceive the data via the sensor parameters.</p> <p>The task on the other hand is an interpretation of the simulation world and the information provided by / collected from it to reach a particular goal that is deired by the user. The same environment can be used to train multiple tasks and the tasks can be changed without changing an environment definition.</p> <p>For example, an empty environment with a quadrotor can be used to train a position setpoint task, or a trajectory tracking task. An environment with a set of obstacles can be used to train a policy that can either navigate through the obstacles or perch on a specific asset in the environment. The task is the interpretation of the environment data for the RL algorithm to learn the desired behavior.</p> <p>To relate it with a familiar environment from the OpenAI Gym suite of tasks, an \"environment\" in our case could refer to a CartPole world with the dynamics of the CartPole, however a \"task\" in our case would allow the same cartpole to be controlled to balance the pole upright, or to keep swinging the pole at a given angular rate or to have the endpoint of the pole at a given location in the environment. All of which require different formulation of rewards and observations for the RL algorithm to learn the desired behavior.</p>"},{"location":"7_FAQ_and_troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>My Isaac Gym viewer window does not show anything or is blank</p> <p>This can be because of a discrepancy in the version of your NVIDIA drivers. Please ensure that you have the appropriate NVIDIA drivers on your system as per the Isaac Gym documentation, and that the Isaac Gym examples such as <code>1080_balls_of_solitude.py</code> and <code>joint_monkey.py</code> are working as expected. Please also make sure that the environment variables <code>LD_LIBRARY_PATH</code> and <code>VK_ICD_FILENAMES</code> are set correctly.</p> <p>rgbImage buffer error 999</p> <pre><code>[Error] [carb.gym.plugin] cudaImportExternalMemory failed on rgbImage buffer with error 999\n</code></pre> <p>This is most likely due to an improper Vulkan configuration. Please refer to the Troubleshooting section of the Isaac Gym documentation and check if the <code>VK_ICD_FILENAMES</code> environment variable is set, and that the file exists.</p> <p>My simulation assets are going through each other without collisions</p> <p>This occurs in case of an improperly set quaternion for the simulation assets. Kindly check that the quaternion is normalized as required by the Isaac Gym Simulator, and that it is in the format <code>[q_x, q_y, q_z, q_w]</code>.</p> <p>My simulation assets just spin very fast at each reset</p> <p>There seem to be <code>nan</code> values somewhere in the measurements of your implementation. This can be due to a variety of reasons, such as improper quaternion normalization, or improper sensor measurements. Please check your code and ensure that all measurements are valid and within the expected range.</p> <p>I see an error that ends with <code>if len(self._meshes) == 0:</code></p> <p>Most likely you have installed urdfpy package via pip. This version has a bug that has since been resolved in the <code>master</code> branch of the URDFPY project repository. Please follow the installation page to install the package from source.</p>"},{"location":"8_sensors_and_rendering/","title":"Sensors and Rendering","text":""},{"location":"8_sensors_and_rendering/#enabling-sensors-in-simulation","title":"Enabling Sensors in Simulation","text":"<p>The sensors can be chosen and enabled or disabled before each robot by editing the robot config file: <pre><code>class sensor_config:\n    enable_camera = True\n    camera_config = BaseDepthCameraConfig\n\n    enable_lidar = False\n    lidar_config = BaseLidarConfig  # OSDome_64_Config\n\n    enable_imu = False\n    imu_config = BaseImuConfig\n</code></pre></p>"},{"location":"8_sensors_and_rendering/#warp-sensors","title":"Warp Sensors","text":"<p>We develop a set of sensors using NVIDIA Warp. We create custom implementations of ray-casting based sensors to obtain depth, range, pointcloud, segmentation and surface normal information from the environment. The various sensors that are provided out-of-the-box are listed below, along with the customization options that are available.</p> <p>Some common configurable parameters for the sensors are described below:</p> <pre><code>height = 128 # height of image or number of scan lines in the LiDAR sensor\nwidth = 512 # width of the image or number of points per scan line in the LiDAR sensor\nhorizontal_fov_deg_min = -180 # minimum horizontal FoV in degrees\nhorizontal_fov_deg_max = 180 # maximum horizontal FoV in degrees\nvertical_fov_deg_min = -45 # minimum vertical FoV in degrees\nvertical_fov_deg_max = +45 # maximum vertical FoV in degrees\nmax_range = 10.0 # maximum range\nmin_range = 0.2 # minimum range\ncalculate_depth = True # Calculate depth image / setting false will return range. (Only for camera sensors)\nreturn_pointcloud = False  # Return a pointcloud [x,y,z] instead of an image\npointcloud_in_world_frame = False # Pointcloud are expressed in the robot or world frame\nsegmentation_camera = True  # Also return a segmentation image\nnormalize_range = True  # normalize values 0 and 1 w.r.t sensor range limits.\n</code></pre>"},{"location":"8_sensors_and_rendering/#depth-camera","title":"Depth Camera","text":"<p>The depth camera sensor provides depth information about the environment. The sensor can be configured with different field of view (FOV) and resolution settings. The depth camera can be used to generate depth images, point clouds, and surface normals.</p> <p>For this sensor model, the vertical and horizontal FOV are related to each other with the relation</p> \\[\\textrm{vfov} = 2 \\arctan(\\tan(\\textrm{hfov}/2) * \\textrm{height}/\\textrm{width}).\\] <p>The depth image is then returned as a  tensor <code>[num_envs, num_sensor_per_robot, vertical_pixels, horizontal_pixels]</code> where the last two dimensions are the vertical and horizontal pixels of the depth image. The Aerial Gym Simulator can also return the pointcloud of the environment on setting <code>calculate_pointcloud = True</code> in the sensor configuration file. This is returned as a 4D tensor <code>[num_envs, num_sensor_per_robot, vertical_pixels, horizontal_pixels, 3]</code> where the last dimension is the 3D coordinates of the point.</p> <p>Note</p> <p>When a range / depth image is returned, the information is returned in the dimensions <code>[num_envs, num_sensor_per_env, vertical_pixels, horizontal_pixels]</code>, however in case the segmentation camera option is selected, another tensor with the same dimensions is initialized to store this information. In case the option to return data as pointcloud is selected, the data is returned as a 4D tensor <code>[num_envs, num_sensor_per_env, vertical_pixels, horizontal_pixels, 3]</code> where the last dimension is the 3D coordinates of the point in the world frame.</p> <p>Some examples of the depth image obtained using this sensor is shown below:</p> <p> </p>"},{"location":"8_sensors_and_rendering/#lidar","title":"Lidar","text":"<p>A LiDAR sensor is modeled after the configuration of an Ouster OS0-128 sensor. The sensor can be configured with a different FoV, resolution, range and noise settings. The sensor allows to return a range-image and the pointclouds of the environments, along with segmentation images and also the surface normals. Similar to many popular existing implementations, this implementation performs ray-casting of all the rays after the physics step is performed. This does not take into account the spinning of the lidar and the change in the robot position during one scan of the lidar.</p> <p>An idealized lidar sensor is used to perform ray-casting. The ray direction for each ray is calculated using the horizontal and vertical FOV of the sensor. The ray direction is calculated as follows:</p> \\[ \\textrm{ray}[i, j] = \\begin{bmatrix} \\cos(\\phi) \\times \\cos(\\theta) \\\\ \\sin(\\phi) \\times \\cos(\\theta) \\\\ \\sin(\\theta) \\end{bmatrix}, \\] <p>where \\(\\phi\\) is the azimuth angle and \\(\\theta\\) is the elevation angle. The azimuth and elevation angles range from the maximum to the minimum FOV of the sensor in the horizontal and vertical directions respectively.</p> Code for initializing ray directions for a LiDAR Sensor <pre><code>for i in range(self.num_scan_lines):\n    for j in range(self.num_points_per_line):\n        # Rays go from +HFoV/2 to -HFoV/2 and +VFoV/2 to -VFoV/2\n        azimuth_angle = self.horizontal_fov_max - (\n            self.horizontal_fov_max - self.horizontal_fov_min\n        ) * (j / (self.num_points_per_line - 1))\n        elevation_angle = self.vertical_fov_max - (\n            self.vertical_fov_max - self.vertical_fov_min\n        ) * (i / (self.num_scan_lines - 1))\n        ray_vectors[i, j, 0] = math.cos(azimuth_angle) * math.cos(elevation_angle)\n        ray_vectors[i, j, 1] = math.sin(azimuth_angle) * math.cos(elevation_angle)\n        ray_vectors[i, j, 2] = math.sin(elevation_angle)\n# normalize ray_vectors\nray_vectors = ray_vectors / torch.norm(ray_vectors, dim=2, keepdim=True)\n</code></pre> <p>For both the Depth Camera and the LiDAR sensors, the measurements can be returned as range and/or depth images and also as pointclouds. The pointclouds can be returned in the world frame or the sensor frame by setting the parameter <code>pointcloud_in_world_frame = True</code> in the sensor configuration file.</p> <p>Sample LiDAR renderings are shown below:</p> <p> </p>"},{"location":"8_sensors_and_rendering/#segmentation-camera","title":"Segmentation Camera","text":"<p>The segmentation camera is provided combined with the depth or the LiDAR sensor. The segmentation camera provides a segmentation image of the environment. In the Isaac Gym rendering framework, the segmentation information can be embedded in each link of the asset in the environment, however for possibility of faster rendering and more flexibility, we allow our Warp environment representation to include the segmentation information per vertex of the mesh. Practically, this method hijacks the velocity field of each vertex in the mesh to encode the segmentation information. The segmentation information is then queried from a particular vertex of each face intersecting the ray cast from the sensor origin. While we provide the capbility equivalent to what is provided in the Isaac Gym rendering framework, this can easily be extended to include more information for the mesh or to have more complex segmentation information that can be assigned to vertices associated with particular faces (triangles) in the mesh.</p> <p>The segmentation camera is associated with the depth/range or the LiDAR sensor, and can only be enabled by setting a flag <code>segmentation_camera = True</code> in the sensor configuration file. Moreover, the segmentation camera queries the mesh directly to read the <code>velocities</code> field of the vertex data and use the first element (x-velocity field) of the first vertex of the face to encode and read the segmentation information from the mesh face.</p> Segmentation Camera Kernel Code <pre><code>if wp.mesh_query_ray(\n    mesh, ray_origin, ray_direction_world, far_plane, t, u, v, sign, n, f\n):\n    dist = t\n    mesh_obj = wp.mesh_get(mesh)\n    face_index = mesh_obj.indices[f * 3]\n    segmentation_value = wp.int32(mesh_obj.velocities[face_index][0])\nif pointcloud_in_world_frame:\n    pixels[env_id, cam_id, scan_line, point_index] = (\n        ray_origin + dist * ray_direction_world\n    )\nelse:\n    pixels[env_id, cam_id, scan_line, point_index] = dist * ray_dir\nsegmentation_pixels[env_id, cam_id, scan_line, point_index] = segmentation_value\n</code></pre> <p>The segmentation camera's output for the corresponding depth images for the camera are shown below:</p> <p> </p> <p> </p> <p>Similarly, for the LiDAR sensor, the range and segmentation image outputs are shown below:</p> <p> </p> <p> </p>"},{"location":"8_sensors_and_rendering/#custom-sensors","title":"Custom Sensors","text":"<p>While the framework to implement the Depth Cameras and the LiDAR sensors is provided, it can be modified to simulate custom sensors as per the user's needs. For example, ToF cameras and single ray range sensors can be designed and used in the simulator. Alternatively, the implementations provided can be modified to obtain exclusively segmentation information or the scene flow information from the simulator.</p>"},{"location":"8_sensors_and_rendering/#randomizing-sensor-placement","title":"Randomizing Sensor Placement","text":"<p>Moreover, the sensors can have randomized placement and orientation. By default, this happens at each <code>&lt;sensor&gt;.reset()</code>, however, as opposed to Isaac Gym sensors, this can also be done at each timestep by the user without introducing any slowdown.  The parameters for randomizing the placement and orientation of the sensors are shown below, and can be changed in the respective config files:</p> Randomizing Sensor Placement and Orientation <pre><code># randomize placement of the sensor\nrandomize_placement = True\nmin_translation = [0.07, -0.06, 0.01] # [m] # Max translation limits of the sensor for [x, y, z] axes\nmax_translation = [0.12, 0.03, 0.04] # [m] # Minimum translation limits of the sensor for [x, y, z] axes\nmin_euler_rotation_deg = [-5.0, -5.0, -5.0] # [deg] # Minimum rotation limits of the sensor for [x, y, z] axes\nmax_euler_rotation_deg = [5.0, 5.0, 5.0] # [deg] # Maximum rotation limits of the sensor for [x, y, z] axes\n\n# nominal position and orientation (only for Isaac Gym Camera Sensors)\n# this is done as randomizing the placement of the camera at each reset is slow\nnominal_position = [0.10, 0.0, 0.03] # [m] # Nominal position of the sensor in the robot frame\nnominal_orientation_euler_deg = [0.0, 0.0, 0.0] # [deg] # Nominal orientation of the sensor in the robot frame\n</code></pre>"},{"location":"8_sensors_and_rendering/#simulating-sensor-noise","title":"Simulating sensor noise","text":"<p>We provide two sensor noise models. The first is a pixel dropout model where a pixel in the image is randomly set to zero with a probability <code>pixel_dropout_prob</code>. The second is a pixel noise model where the pixel values are perturbed by a Gaussian noise with a standard deviation of <code>pixel_std_dev_multiplier</code> times the pixel value. The parameters for the sensor noise models are shown below:</p> <pre><code>class sensor_noise:\n    enable_sensor_noise = False\n    pixel_dropout_prob = 0.01\n    pixel_std_dev_multiplier = 0.01\n</code></pre> <p>Should I use Warp or Isaac Gym sensors?</p> <p>This depends on your use case. If you are simulating dynamic environments, Isaac Gym sensors are required. If you are using an implementation that is provided by both frameworks and you only want simulation speed, it is highly recommended to use warp. As a general observation, Warp worked faster generally and we specifically saw orders of magnitude speedups while simulating single or multiple low-dimensional sensors (such as 8x8 ToF sensors) per robot, while the Isaac Gym sensor was slightly better for our case when dealing with many complex meshes in the environment.</p> <p>However Isaac Gym sensors cannot simulate LiDARs or any custom sensor model. If you need better customizability and capability to randomize sensor paramters such as pose, or projection model of the sensors on-the-fly, then our Warp implementation is recommended. Here, the position, orientation, camera matrix, ray casting direction etc can be changed or randomized at each timestep (whoaaa!!!) without introducing much delays. If you want to embed additional information in your environment itself that can be queried by the sensors, our Warp implementation is a natural optionas this allows for an immensely powerful rendering framework.</p>"},{"location":"8_sensors_and_rendering/#isaac-gym-camera-sensors","title":"Isaac Gym Camera sensors","text":"<p>Using both Isaac Gym and Warp rendering</p> <p>Enabling both Warp and Isaac Gym rendering pipelines together can cause a slowdown in the simulation and has not been extensively tested. It is recommended to use one rendering pipeline at a time.</p> <p>We provide wrappers to enable the use of Isaac Gym cameras with the robots in the simulator. These camera sensors are provided as-is from the Isaac Gym interface. The sensors offered include RGB, Depth, and Segmentation and Optical Flow cameras. Isaac Gym Simulator does not provide all the customization options that we offer with the Warp-based sensors. However, we provide a standardized interface to use these sensors with the robots in the simulator.</p>"},{"location":"8_sensors_and_rendering/#imu-sensor","title":"IMU Sensor","text":"<p>An IMU sensor is implemented that can calculate the acceleration and angular rates of the robot. The sensor is configured by default to be mounted at the origin of the base link of the robot, while the orientation is configurable. This is done as the IMU is implemented using the Force sensor in Isaac Gym and cannot account for centripetal forces and gyroscopic effects. The IMU measurement is obtained as follows:</p> \\[ a_{\\textrm{meas}} = a_{\\textrm{true}} + b_a + n_a, \\] \\[ \\omega_{\\textrm{meas}} = \\omega_{\\textrm{true}} + b_{\\omega} + n_{\\omega}, \\] <p>where, \\(a_{\\textrm{meas}}\\) and \\(\\omega_{\\textrm{meas}}\\) are the measured acceleration and angular rates, \\(a_{\\textrm{true}}\\) and \\(\\omega_{\\textrm{true}}\\) are the true acceleration and angular rates, \\(b_a\\) and \\(b_{\\omega}\\) are the biases, and \\(n_a\\) and \\(n_{\\omega}\\) are the noise terms. The noise terms are modeled as Gaussian noise with standard deviations \\(\\sigma_{n_a}\\) and \\(\\sigma_{n_{\\omega}}\\) respectively. The biases are modeled as random walk processes with parameters \\(\\sigma_{a}\\) and \\(\\sigma_{\\omega}\\) respectively.</p> The bias model for the IMU <p>is a random walk model and the parameters set for the provided sensor are obtained from a VN-100 IMU. Using an IMU requires the force sensor to be enabled on the robot asset configuration.</p> \\[ b_{a,k} = b_{a,k-1} + \\sigma_{a} \\cdot \\mathcal{N}(0,1) / \\sqrt{\\Delta t}, \\] \\[ b_{\\omega,k} = b_{\\omega,k-1} + \\sigma_{\\omega} \\cdot \\mathcal{N}(0,1) \\cdot \\sqrt{\\Delta t}, \\] <p>where \\(b_{a,k}\\) and \\(b_{\\omega,k}\\) are the biases at time \\(k\\), \\(\\sigma_{a}\\) and \\(\\sigma_{\\omega}\\) are the bias random walk parameters, \\(\\Delta t\\) is the time step, \\(a_{\\textrm{meas}}\\) and \\(\\omega_{\\textrm{meas}}\\) are the measured acceleration and angular rates, \\(a_{\\textrm{true}}\\) and \\(\\omega_{\\textrm{true}}\\) are the true acceleration and angular rates, \\(b_a\\) and \\(b_{\\omega}\\) are the biases, The noise terms \\(n_a\\) and \\(n_{\\omega}\\) are modeled as:</p> \\[ n_a = \\sigma_{n_a} \\cdot \\mathcal{N}(0,1) / \\sqrt{\\Delta t}, \\textrm{and} \\] \\[ n_{\\omega} = \\sigma_{n_{\\omega}} \\cdot \\mathcal{N}(0,1) / \\sqrt{\\Delta t}. \\]"},{"location":"9_sim2real/","title":"Sim-2-real","text":""},{"location":"9_sim2real/#px4-module","title":"PX4 Module","text":"<p>To simplify deploying the networks to a platform, an experimental module in px4 has been created along with a guide. On this page you can read about training, network conversion, uploading and building PX4 to test it out for yourself, or build upon it for further research or use-cases.</p> <p>This example uses the ARL LMF drone platform, by doing a hover flight and measuring a couple of things on your own platform you can customize the sim to train networks specifically for your platform</p> <p>To train your own networks for use in PX4, start by following the installation instructions. Then you can go into the rl_games folder and run the runner.py script.</p> <pre><code>cd aerial_gym/rl_training/rl_games\npython runner.py --task=position_setpoint_task_sim2real_end_to_end\n</code></pre> <p>If you want to customize the simulator to your own platform or research please check the Customizing the Simulator section. The relevant files can be found in:</p> <ul> <li>Robot:<ul> <li>Configuration: aerial_gym/config/robot_config/lmf1_config.py</li> <li>Model: resources/robots/lmf1/model.urdf</li> </ul> </li> <li>Network size, layers and activation function: aerial_gym/rl_training/rl_games/ppo_aerial_quad.yaml</li> <li>Task:<ul> <li>Configuration: aerial_gym/config/task_config/position_setpoint_task_sim2real_end_to_end_config.py</li> <li>Task: aerial_gym/task/position_setpoint_task_sim2real_end_to_end/position_setpoint_task_sim2real_end_to_end.py</li> </ul> </li> </ul> <p>In the task you can change the actions, inputs, reward functions, which robot to use etc. Make sure the lmf1 robot is chosen in the task config if you want to use that as a starting point.</p>"},{"location":"9_sim2real/#optimizing-for-your-platform","title":"Optimizing for your platform","text":"<p>To train a optimal position setpoint controller for your own platform you should change the robot files. These are the most important changes to make sure the platform in sim is close to the real platform:</p> <ul> <li>The total weight of the platform in flight, including batteries. This is changed in the urdf file as the mass of the base_link.</li> <li>The inertia of the platform, this needs to be calculated using standard methods from measurements of the platform, or a CAD file. Added in the urdf file under the inertia of the base link</li> <li>The positions of the different motors, how far they are from the middle of the platform. This is changed in the urdf file under each of the base_link_to_..._prop as origin xyz.</li> <li>Motor time constants, usually found in the datasheet of the motors. Added in the robot config file as motor_time_constant...</li> <li>Motor thrust coefficients, found by doing a hover flight and logging the rpm values needed to hover. The calculation is 9.81*platform weight/4 = thrust per motor to keep the drone hovering. Thrust / (rpm^2) = motor thrust coefficient. Then change the values in the robot config file motor_thrust_constant... You can also use rps or rads/s instead of rpm by changing the motor config.</li> </ul>"},{"location":"9_sim2real/#conversion","title":"Conversion","text":"<p>While the Aerial Gym Simulator uses the PyTorch framework, the PX4 module uses TensorFlow Lite Micro (TFLM). Therefore the networks trained in Aerial Gym needs to be converted before they can be added into PX4. Along with the instructions here, a conversion script can be found in the resources/conversion folder.</p> <ol> <li> <p>First of all you need to setup the conversion environment. There are some packages here that interferes with the Aerial Gym ones so the recommended way is to exit the conda environment and create a python virtual environment:</p> <pre><code>cd resources/conversion\npython -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre> </li> <li> <p>Then you need to copy in the networks. If you are using the example above you will find the networks in aerial_gym/rl_training/rl_games/runs/{your newest run}/nn/gen_ppo.pth</p> </li> <li> <p>Open the convert.py script and make sure the layer sizes and activation functions match the network you have trained. If you have an example input where you know the correct outputs, you can switch out the sample_input variable to make sure the tflite model produces the correct outputs. Also make sure the correct model name is specified for the network you have copied in.</p> </li> <li> <p>Run the conversion script</p> <pre><code>python convert.py\n</code></pre> </li> <li> <p>Check that the outputs are correct</p> </li> <li> <p>Run this command with the correct file names: (This is a linux command)</p> <pre><code>xxd -i gen_ppo.tflite &gt; gen_ppo.cc\n</code></pre> </li> <li> <p>To use the new network, copy the numbers in the array and just this, the declaration of the network and the definition needs to be as it is in the PX4 module! Paste these over the array elements in src/modules/mc_nn_control/control_net.cpp, then take the size in the bottom of the gen_ppo.cc file and replace the size in the header file; src/modules/mc_nn_control/control_net.hpp.</p> </li> </ol>"},{"location":"9_sim2real/#installing-px4","title":"Installing PX4","text":"<p>The module is currently not available in the main PX4 repo, because we are waiting for a toolchain upgrade. In the meantime the code can be found in this fork and branch. If anything fail with the first three steps, check the PX4 docs.</p> <ol> <li> <p>First clone the branch with its submodules</p> <pre><code>git clone --recurse-submodules -b for_paper https://github.com/SindreMHegre/PX4-Autopilot-public.git\n</code></pre> </li> <li> <p>Fetch the tags from the main PX4 repo, PX4 will not build without them</p> <pre><code>git fetch upstream --tags\n</code></pre> </li> <li> <p>Run the toolchain installation script, note that this may break other packages on your computer</p> <pre><code>bash ./Tools/setup/ubuntu.sh\n</code></pre> </li> <li> <p>Now for the neural part. Add TFLM as a submodule</p> <pre><code>git submodule add -b main https://github.com/tensorflow/tflite-micro.git src/lib/tflm/tflite_micro/\n</code></pre> </li> <li> <p>Then we need to install the TFLM dependencies. This is automatically done when you build it as a static library, enter the tflite-micro folder and do the following command:</p> <pre><code>cd src/lib/tflm/tflite_micro\n</code></pre> <pre><code>make -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex_m_generic TARGET_ARCH=cortex-m7 microlite\n</code></pre> </li> <li> <p>While this is building (it can take a couple of minutes) we can some other changes. The toolchain file in platforms/nuttx/cmake/Toolchain-arm-none-eabi.cmake needs to be edited. In this file you need to add your local path to the PX4-Autopilot repo. This line is marked with a TODO comment.</p> </li> <li> <p>PX4 excludes standard libraries by default, if they are enabled they will break the nuttx build. To get around this we extract some of the standard library header files. This needs to be done after the TFLM make command is finished.</p> <pre><code>cd src/lib/tflm\ncp -r tflite_micro/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/arm-none-eabi/include/c++/13.2.1/ include\nrm include/13.2.1/arm-none-eabi/bits/ctype_base.h\ncp ../../modules/mc_nn_control/setup/ctype_base.h include/13.2.1/arm-none-eabi/bits/\ncd ../../..\n</code></pre> </li> <li> <p>(Optional) If you want to include the neural network controller module onto a new board, add:</p> <p><pre><code>CONFIG_MODULES_MC_NN_CONTROL=y\n</code></pre> to your .px4board file. There are three pre-made board config files where other modules are removed to make sure the entire executable fits in the flash memory of the boards. These are: px4_sitl_neural, px4_fmu-v6c_neural and mro_pixracerpro_neural</p> </li> <li> <p>Now everything should be set up and you can build it using the standard make commands</p> <pre><code>make px4_sitl_neural\n</code></pre> </li> </ol> <p>Warning! When switching to the neural controller on an actual drone there has been a bug some times that the network produces NAN as motor outputs if it does not receive trajectory setpoints. It is advised to change the mc_nn_testing module to setpoints that you desire and start that module first.</p>"}]}